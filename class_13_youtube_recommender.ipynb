{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mature-projection",
   "metadata": {},
   "source": [
    "# YouTube Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "verified-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display, HTML\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Fix the dying kernel problem (only a problem in some installations - you can remove it, if it works without it)\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-catholic",
   "metadata": {},
   "source": [
    "# Candidate Generation Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-assessment",
   "metadata": {},
   "source": [
    "**Task 1.** Code the Deep Candidate Generation Network as described in the Covington, Adams, Sargin, Deep Neural Networks for YouTube Recommendations. The network should accept the following parameters in the init method:\n",
    "\n",
    "- n_items - dictionary size (the number of all items), \n",
    "- embedding_dim - item embedding dimension, \n",
    "- hidden_dim - last fully connected hidden layer size (user embedding dim), \n",
    "- n_hidden_layers - the number of hidden layers, \n",
    "- seed - seed for the random number generator.\n",
    "\n",
    "The input to the network has the following form:\n",
    "\n",
    "```\n",
    "user_histories = [\n",
    "    torch.LongTensor([7, 3, 9]),\n",
    "    torch.LongTensor([11]),\n",
    "    torch.LongTensor([0]),\n",
    "    torch.LongTensor([10, 1, 4, 8])\n",
    "]\n",
    "```\n",
    "\n",
    "It's a batch of item interaction histories for several users. The histories can have different lengths.\n",
    "\n",
    "The architecture of the network should be as follows:\n",
    "\n",
    "- In the first step you can loop over elements of the batch.\n",
    "- The embedding layer should be applied to every user's history.\n",
    "- An average embedding should be calculated for the entire user's history.\n",
    "- The entire batch of those averaged embeddings should be fed forward through hidden layers. There should be n_hidden_layers hidden layers. The last hidden layer should have hidden_dim output neurons. Every previous hidden layer should have twice as many output neurons as the next hidden layer. The first hidden layer must have the input dimension compatible with the result of operations described in previous bullets.\n",
    "- The final layer should have hidden_dim input neurons and one output neuron per item. Finally, softmax should be applied on this layer.\n",
    "- All layers should not have the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "supposed-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCandidateGenerationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Extreme multi-class classifier network.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_items, embedding_dim, hidden_dim, n_hidden_layers, seed):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        self.fc = nn.ModuleList([nn.Linear(embedding_dim, hidden_dim * 2**(n_hidden_layers - 1), bias=False)])\n",
    "        \n",
    "        for layer in range(n_hidden_layers - 1, 0, -1):\n",
    "            self.fc.append(\n",
    "                nn.Linear(hidden_dim * 2**layer, hidden_dim * 2**(layer - 1), bias=False)\n",
    "            )\n",
    "                \n",
    "        self.fc_classifier = nn.Linear(hidden_dim, n_items, bias=False)\n",
    "            \n",
    "\n",
    "    def forward(self, user_histories):\n",
    "        results = []\n",
    "        \n",
    "        for history in user_histories:\n",
    "            item_embeddings = self.item_embedding(history)\n",
    "\n",
    "            avg_embedding = item_embeddings.mean(dim=0)\n",
    "            x = avg_embedding\n",
    "\n",
    "            for fc in self.fc:\n",
    "                x = torch.relu(fc(x))\n",
    "            \n",
    "            results.append(x)\n",
    "        \n",
    "        x = torch.stack(results)\n",
    "        x = torch.softmax(self.fc_classifier(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "administrative-elephant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepCandidateGenerationModel(\n",
      "  (item_embedding): Embedding(12, 3)\n",
      "  (fc): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=48, bias=False)\n",
      "    (1): Linear(in_features=48, out_features=24, bias=False)\n",
      "    (2): Linear(in_features=24, out_features=12, bias=False)\n",
      "    (3): Linear(in_features=12, out_features=6, bias=False)\n",
      "  )\n",
      "  (fc_classifier): Linear(in_features=6, out_features=12, bias=False)\n",
      ")\n",
      "tensor([[0.0814, 0.0823, 0.0838, 0.0832, 0.0830, 0.0832, 0.0818, 0.0833, 0.0843,\n",
      "         0.0858, 0.0843, 0.0834],\n",
      "        [0.0816, 0.0824, 0.0837, 0.0829, 0.0829, 0.0835, 0.0823, 0.0837, 0.0846,\n",
      "         0.0849, 0.0842, 0.0833],\n",
      "        [0.0763, 0.0786, 0.0834, 0.0798, 0.0833, 0.0854, 0.0800, 0.0867, 0.0879,\n",
      "         0.0872, 0.0862, 0.0852],\n",
      "        [0.0813, 0.0819, 0.0833, 0.0823, 0.0835, 0.0839, 0.0823, 0.0843, 0.0844,\n",
      "         0.0845, 0.0841, 0.0841]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "dcg_network = DeepCandidateGenerationModel(n_items=12, embedding_dim=3, hidden_dim=6, n_hidden_layers=4, seed=6789)\n",
    "\n",
    "print(dcg_network)\n",
    "\n",
    "user_histories = [\n",
    "    torch.LongTensor([7, 3, 9]),\n",
    "    torch.LongTensor([11]),\n",
    "    torch.LongTensor([0]),\n",
    "    torch.LongTensor([10, 1, 4, 8])\n",
    "]\n",
    "\n",
    "result = dcg_network(user_histories)\n",
    "print(result)\n",
    "\n",
    "assert (np.round(np.array(result.tolist()), 4) == np.round(\n",
    "    np.array([[0.0814482569694519, 0.08233463764190674, 0.08382046967744827, 0.08316171914339066, 0.08304114639759064, \n",
    "               0.08322826772928238, 0.08181853592395782, 0.08334977924823761, 0.08427998423576355, 0.0858381986618042, \n",
    "               0.08428454399108887, 0.08339447528123856], \n",
    "              [0.08161208778619766, 0.0823715552687645, 0.08370236307382584, 0.0829075276851654, 0.08293760567903519, \n",
    "               0.08351287990808487, 0.08229964226484299, 0.08368352055549622, 0.08461923152208328, 0.08492446690797806, \n",
    "               0.08417494595050812, 0.08325410634279251], \n",
    "              [0.07629935443401337, 0.07855911552906036, 0.08342088758945465, 0.07980967313051224, 0.08325998485088348, \n",
    "               0.08542384952306747, 0.07999198138713837, 0.086661696434021, 0.08794154226779938, 0.0872378796339035, \n",
    "               0.08619119971990585, 0.08520283550024033], \n",
    "              [0.08132115006446838, 0.08194458484649658, 0.08329392224550247, 0.08231865614652634, 0.08351138979196548, \n",
    "               0.08392892777919769, 0.08234265446662903, 0.08428220450878143, 0.08438169211149216, 0.0844755545258522, \n",
    "               0.08409841358661652, 0.08410079777240753]]), 4)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-adelaide",
   "metadata": {},
   "source": [
    "# Ranking Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-dairy",
   "metadata": {},
   "source": [
    "**Task 2.** Code the Deep Ranking Network as described in the Covington, Adams, Sargin, Deep Neural Networks for YouTube Recommendations. The network should accept the following parameters in the init method:\n",
    "\n",
    "- n_items - dictionary size (the number of all items), \n",
    "- embedding_dim - item embedding dimension, \n",
    "- hidden_dim - last fully connected hidden layer size (user embedding dim), \n",
    "- n_hidden_layers - the number of hidden layers, \n",
    "- seed - seed for the random number generator.\n",
    "\n",
    "The input to the network has the following form:\n",
    "\n",
    "```\n",
    "user_history = torch.LongTensor([7, 3, 9])\n",
    "\n",
    "scored_item = torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
    "```\n",
    "\n",
    "It's an item interaction history for a single user and a batch of item ids to be scored.\n",
    "\n",
    "The architecture of the network should be as follows:\n",
    "\n",
    "- The embedding layer should be applied to user's history.\n",
    "- An average embedding should be calculated for user's history.\n",
    "- The embedding layer should be applied to the batch of scored items.\n",
    "- The user's history average embedding should be concatenated from the left side to every scored item embedding in the batch.\n",
    "- The entire batch of those concatenated embeddings should be fed forward through hidden layers. There should be n_hidden_layers hidden layers. The last hidden layer should have hidden_dim output neurons. Every previous hidden layer should have twice as many output neurons as the next hidden layer. The first hidden layer must have the input dimension compatible with the result of operations described in previous bullets. Hidden layers should not have the bias term.\n",
    "- The final layer should have hidden_dim input neurons and one output neuron. This layer should have the bias term. Finally, sigmoid should be applied on this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "constant-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRankingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Ranking network.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_items, embedding_dim, hidden_dim, n_hidden_layers, seed):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        self.fc1 = nn.Linear(2 * embedding_dim, hidden_dim, bias=False)\n",
    "        self.fc = nn.ModuleList([nn.Linear(embedding_dim * 2, hidden_dim * 2**(n_hidden_layers - 1), bias=False)])\n",
    "        \n",
    "        for layer in range(n_hidden_layers - 1, 0, -1):\n",
    "            self.fc.append(nn.Linear(hidden_dim * 2**layer, hidden_dim * 2**(layer - 1), bias=False))\n",
    "            \n",
    "        self.fc_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, user_history, scored_item):\n",
    "        item_embeddings = self.item_embedding(user_history)\n",
    "        avg_embedding = item_embeddings.mean(dim=0)\n",
    "        scored_embeddings = self.item_embedding(scored_item)\n",
    "        \n",
    "        x = torch.cat([avg_embedding.expand(scored_embeddings.size(dim=0), -1), scored_embeddings], dim=1)\n",
    "        \n",
    "        for fc in self.fc:\n",
    "            x = torch.relu(fc(x))\n",
    "            \n",
    "        x = torch.sigmoid(self.fc_final(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "framed-connectivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepRankingModel(\n",
      "  (item_embedding): Embedding(12, 3)\n",
      "  (fc1): Linear(in_features=6, out_features=6, bias=False)\n",
      "  (fc): ModuleList(\n",
      "    (0): Linear(in_features=6, out_features=48, bias=False)\n",
      "    (1): Linear(in_features=48, out_features=24, bias=False)\n",
      "    (2): Linear(in_features=24, out_features=12, bias=False)\n",
      "    (3): Linear(in_features=12, out_features=6, bias=False)\n",
      "  )\n",
      "  (fc_final): Linear(in_features=6, out_features=1, bias=True)\n",
      ")\n",
      "tensor([[0.4069],\n",
      "        [0.4079],\n",
      "        [0.4055],\n",
      "        [0.4080],\n",
      "        [0.4106],\n",
      "        [0.4066],\n",
      "        [0.4061],\n",
      "        [0.4100],\n",
      "        [0.4036],\n",
      "        [0.4062],\n",
      "        [0.4094],\n",
      "        [0.4100]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "dr_network = DeepRankingModel(n_items=12, embedding_dim=3, hidden_dim=6, n_hidden_layers=4, seed=6789)\n",
    "\n",
    "print(dr_network)\n",
    "\n",
    "user_history = torch.LongTensor([7, 3, 9])\n",
    "\n",
    "scored_item = torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
    "\n",
    "result = dr_network(user_history, scored_item)\n",
    "print(result)\n",
    "\n",
    "assert all(np.round(np.array(result.tolist()), 4) == np.round(\n",
    "    np.array([[0.4069308638572693], [0.407929003238678], [0.4055391550064087], [0.40804415941238403], [0.4105871021747589], \n",
    "              [0.4065965712070465], [0.40611234307289124], [0.40995174646377563], [0.40362346172332764], \n",
    "              [0.4062325656414032], [0.4094473421573639], [0.41004639863967896]]), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-plymouth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
